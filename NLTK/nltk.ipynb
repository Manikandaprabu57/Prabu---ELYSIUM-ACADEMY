{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***NATURAL LANGUAGE TOOL KIT***"
      ],
      "metadata": {
        "id": "Q1NBcvTrvL4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwH_y1B9tRPD",
        "outputId": "a574f4d1-f0d0-4983-8cfc-e45b0e31fca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRoSG0Y3rsM4",
        "outputId": "43c99cfe-2d19-4b56-9433-b6ce2bb428fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['waste of time']\n",
            "Words: ['waste', 'of', 'time']\n",
            "POS Tags: [('waste', 'NN'), ('of', 'IN'), ('time', 'NN')]\n",
            "Lemmatized Words: ['waste', 'of', 'time']\n",
            "Sentiment Analysis:\n",
            "  Polarity: -0.2\n",
            "  Subjectivity: 0.0\n",
            "The sentiment of the text is negative.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Function to get the WordNet POS tag from the NLTK POS tag\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Sample text\n",
        "text = \"waste of time\"\n",
        "\n",
        "# Tokenize sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\", sentences)\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(text)\n",
        "print(\"Words:\", words)\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = pos_tag(words)\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatized_words = []\n",
        "for word, tag in pos_tags:\n",
        "    wn_tag = get_wordnet_pos(tag)\n",
        "    lemmatized_word = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "    lemmatized_words.append(lemmatized_word)\n",
        "\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n",
        "\n",
        "# Sentiment Analysis\n",
        "blob = TextBlob(text)\n",
        "sentiment = blob.sentiment\n",
        "print(\"Sentiment Analysis:\")\n",
        "print(f\"  Polarity: {sentiment.polarity}\")\n",
        "print(f\"  Subjectivity: {sentiment.subjectivity}\")\n",
        "\n",
        "# Explanation of Sentiment Analysis\n",
        "if sentiment.polarity > 0:\n",
        "    print(\"The sentiment of the text is positive.\")\n",
        "elif sentiment.polarity < 0:\n",
        "    print(\"The sentiment of the text is negative.\")\n",
        "else:\n",
        "    print(\"The sentiment of the text is neutral.\")"
      ]
    }
  ]
}